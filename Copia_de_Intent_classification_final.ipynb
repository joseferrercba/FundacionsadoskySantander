{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:47.954473Z",
     "iopub.status.busy": "2020-06-30T15:09:47.954473Z",
     "iopub.status.idle": "2020-06-30T15:09:47.976473Z",
     "shell.execute_reply": "2020-06-30T15:09:47.975470Z",
     "shell.execute_reply.started": "2020-06-30T15:09:47.954473Z"
    },
    "id": "a_WypuUXi92e",
    "outputId": "2c54816d-e35a-4c9e-8b7d-0f71093a4eb0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from CustomTokenizer import CustomTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:47.978472Z",
     "iopub.status.busy": "2020-06-30T15:09:47.977470Z",
     "iopub.status.idle": "2020-06-30T15:09:47.992470Z",
     "shell.execute_reply": "2020-06-30T15:09:47.991471Z",
     "shell.execute_reply.started": "2020-06-30T15:09:47.978472Z"
    },
    "id": "LE6wywJrN2ih"
   },
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    df = pd.read_csv(filename, encoding = \"latin1\", names = [\"Pregunta\", \"Intencion\"], sep='|')\n",
    "    print(df.head())\n",
    "    intent = df[\"Intencion\"]\n",
    "    unique_intent = list(set(intent))\n",
    "    sentences = list(df[\"Pregunta\"])\n",
    "\n",
    "    return (intent, unique_intent, sentences)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:47.995471Z",
     "iopub.status.busy": "2020-06-30T15:09:47.994480Z",
     "iopub.status.idle": "2020-06-30T15:09:48.039497Z",
     "shell.execute_reply": "2020-06-30T15:09:48.038499Z",
     "shell.execute_reply.started": "2020-06-30T15:09:47.994480Z"
    },
    "id": "tF0FQA7gjOCX",
    "outputId": "7ad6242b-cd01-460d-a2d4-d05070842150"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Pregunta  Intencion\n",
      "0                                           Pregunta  Intencion\n",
      "1               como puedo trabajar en santander rio    Cat_102\n",
      "2                pagar tarjeta visa querer reintegro    Cat_350\n",
      "3                      pagar tarjeta naranja sistema    Cat_132\n",
      "4  no se debitÃ³ la primera cuota del plan de bie...    Cat_129\n"
     ]
    }
   ],
   "source": [
    "intent, unique_intent, sentences = load_dataset(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:48.040471Z",
     "iopub.status.busy": "2020-06-30T15:09:48.040471Z",
     "iopub.status.idle": "2020-06-30T15:09:48.055473Z",
     "shell.execute_reply": "2020-06-30T15:09:48.054487Z",
     "shell.execute_reply.started": "2020-06-30T15:09:48.040471Z"
    },
    "id": "O8LLUZlokg0S",
    "outputId": "6c52cf60-7034-4573-ae14-105d7f24e658"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pregunta', 'como puedo trabajar en santander rio', 'pagar tarjeta visa querer reintegro', 'pagar tarjeta naranja sistema', 'no se debitÃ³ la primera cuota del plan de bienes personales y quiero saber por que']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:48.057491Z",
     "iopub.status.busy": "2020-06-30T15:09:48.057491Z",
     "iopub.status.idle": "2020-06-30T15:09:48.071471Z",
     "shell.execute_reply": "2020-06-30T15:09:48.070474Z",
     "shell.execute_reply.started": "2020-06-30T15:09:48.057491Z"
    },
    "id": "MhrziINPGHbW",
    "outputId": "c7192800-5dd1-4149-f1e0-47e12d364947"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jose\n",
      "[nltk_data]     Ferrer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Jose\n",
      "[nltk_data]     Ferrer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:48.073475Z",
     "iopub.status.busy": "2020-06-30T15:09:48.072469Z",
     "iopub.status.idle": "2020-06-30T15:09:48.087487Z",
     "shell.execute_reply": "2020-06-30T15:09:48.086505Z",
     "shell.execute_reply.started": "2020-06-30T15:09:48.073475Z"
    },
    "id": "OmNLu2YSXePb"
   },
   "outputs": [],
   "source": [
    "#define stemmer\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:48.090495Z",
     "iopub.status.busy": "2020-06-30T15:09:48.089473Z",
     "iopub.status.idle": "2020-06-30T15:09:48.103470Z",
     "shell.execute_reply": "2020-06-30T15:09:48.102500Z",
     "shell.execute_reply.started": "2020-06-30T15:09:48.090495Z"
    },
    "id": "j-7q3iG5PKYI"
   },
   "outputs": [],
   "source": [
    "def cleaning(sentences):\n",
    "    words = []\n",
    "    for s in sentences:\n",
    "        clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", s)\n",
    "        w = word_tokenize(clean)\n",
    "        #stemming\n",
    "        words.append([i.lower() for i in w])\n",
    "    \n",
    "    return words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:48.105469Z",
     "iopub.status.busy": "2020-06-30T15:09:48.104471Z",
     "iopub.status.idle": "2020-06-30T15:09:50.100470Z",
     "shell.execute_reply": "2020-06-30T15:09:50.098486Z",
     "shell.execute_reply.started": "2020-06-30T15:09:48.105469Z"
    },
    "id": "p1j2GJgDG6qj",
    "outputId": "e5cc298e-42f1-4e76-cf6c-cc2b08f4d66a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20105\n",
      "[['pregunta'], ['como', 'puedo', 'trabajar', 'en', 'santander', 'rio']]\n"
     ]
    }
   ],
   "source": [
    "cleaned_words = cleaning(sentences)\n",
    "print(len(cleaned_words))\n",
    "print(cleaned_words[:2])  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:50.105473Z",
     "iopub.status.busy": "2020-06-30T15:09:50.104477Z",
     "iopub.status.idle": "2020-06-30T15:09:50.115481Z",
     "shell.execute_reply": "2020-06-30T15:09:50.114469Z",
     "shell.execute_reply.started": "2020-06-30T15:09:50.105473Z"
    },
    "id": "SJCQ_YhBJW7t"
   },
   "outputs": [],
   "source": [
    "def create_tokenizer(words, filters = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~'):\n",
    "    token = Tokenizer(filters = filters)\n",
    "    token.fit_on_texts(words)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:50.117501Z",
     "iopub.status.busy": "2020-06-30T15:09:50.117501Z",
     "iopub.status.idle": "2020-06-30T15:09:50.131480Z",
     "shell.execute_reply": "2020-06-30T15:09:50.130471Z",
     "shell.execute_reply.started": "2020-06-30T15:09:50.117501Z"
    },
    "id": "QJhdIJC5Q3Q6"
   },
   "outputs": [],
   "source": [
    "def max_length(words):\n",
    "    return(len(max(words, key = len)))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:50.133471Z",
     "iopub.status.busy": "2020-06-30T15:09:50.133471Z",
     "iopub.status.idle": "2020-06-30T15:09:50.291469Z",
     "shell.execute_reply": "2020-06-30T15:09:50.290509Z",
     "shell.execute_reply.started": "2020-06-30T15:09:50.133471Z"
    },
    "id": "JWjxPGsZZJNX",
    "outputId": "5b3813a9-f787-4a47-e998-85d936376dd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size = 6117 and Maximum length = 46\n"
     ]
    }
   ],
   "source": [
    "word_tokenizer = create_tokenizer(cleaned_words)\n",
    "vocab_size = len(word_tokenizer.word_index) + 1\n",
    "max_length = max_length(cleaned_words)\n",
    "\n",
    "print(\"Vocab Size = %d and Maximum length = %d\" % (vocab_size, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:50.293470Z",
     "iopub.status.busy": "2020-06-30T15:09:50.292469Z",
     "iopub.status.idle": "2020-06-30T15:09:50.307471Z",
     "shell.execute_reply": "2020-06-30T15:09:50.306509Z",
     "shell.execute_reply.started": "2020-06-30T15:09:50.292469Z"
    },
    "id": "D0TXu2xsR8jq"
   },
   "outputs": [],
   "source": [
    "def encoding_doc(token, words):\n",
    "    return(token.texts_to_sequences(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:50.310494Z",
     "iopub.status.busy": "2020-06-30T15:09:50.309477Z",
     "iopub.status.idle": "2020-06-30T15:09:50.705978Z",
     "shell.execute_reply": "2020-06-30T15:09:50.704979Z",
     "shell.execute_reply.started": "2020-06-30T15:09:50.310494Z"
    },
    "id": "dE92Hk1Va--H"
   },
   "outputs": [],
   "source": [
    "encoded_doc = encoding_doc(word_tokenizer, cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:50.706923Z",
     "iopub.status.busy": "2020-06-30T15:09:50.706923Z",
     "iopub.status.idle": "2020-06-30T15:09:50.721978Z",
     "shell.execute_reply": "2020-06-30T15:09:50.720966Z",
     "shell.execute_reply.started": "2020-06-30T15:09:50.706923Z"
    },
    "id": "fyOzLEboc4LZ"
   },
   "outputs": [],
   "source": [
    "def padding_doc(encoded_doc, max_length):\n",
    "    return(pad_sequences(encoded_doc, maxlen = max_length, padding = \"post\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:50.722923Z",
     "iopub.status.busy": "2020-06-30T15:09:50.722923Z",
     "iopub.status.idle": "2020-06-30T15:09:50.816924Z",
     "shell.execute_reply": "2020-06-30T15:09:50.815925Z",
     "shell.execute_reply.started": "2020-06-30T15:09:50.722923Z"
    },
    "id": "WdejoJrlc-tc"
   },
   "outputs": [],
   "source": [
    "padded_doc = padding_doc(encoded_doc, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:50.817923Z",
     "iopub.status.busy": "2020-06-30T15:09:50.817923Z",
     "iopub.status.idle": "2020-06-30T15:09:50.832956Z",
     "shell.execute_reply": "2020-06-30T15:09:50.831981Z",
     "shell.execute_reply.started": "2020-06-30T15:09:50.817923Z"
    },
    "id": "gDgTCS2KdI2p",
    "outputId": "704e73ea-3b2b-4e7a-85ec-b015ccc18cec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1516,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0],\n",
       "       [  14,   17,  949,    9,   43,  129,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0],\n",
       "       [  36,    5,   38,   77,  467,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0],\n",
       "       [  36,    5, 1041,  322,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0],\n",
       "       [  11,   21,  265,    2,  468,  104,   24,  419,    1, 2432,  302,\n",
       "          18,   13,   16,   19,    4,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_doc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:50.833926Z",
     "iopub.status.busy": "2020-06-30T15:09:50.833926Z",
     "iopub.status.idle": "2020-06-30T15:09:50.849924Z",
     "shell.execute_reply": "2020-06-30T15:09:50.847924Z",
     "shell.execute_reply.started": "2020-06-30T15:09:50.833926Z"
    },
    "id": "3eaSIDi0dNf1",
    "outputId": "632ce128-b3d4-4cf5-c49a-7cf9cc45948f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded docs =  (20105, 46)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of padded docs = \",padded_doc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:50.850926Z",
     "iopub.status.busy": "2020-06-30T15:09:50.850926Z",
     "iopub.status.idle": "2020-06-30T15:09:50.864925Z",
     "shell.execute_reply": "2020-06-30T15:09:50.863923Z",
     "shell.execute_reply.started": "2020-06-30T15:09:50.850926Z"
    },
    "id": "X0rXzenSpgFR"
   },
   "outputs": [],
   "source": [
    "#tokenizer with filter changed\n",
    "output_tokenizer = create_tokenizer(unique_intent, filters = '!\"#$%&()*+,-/:;<=>?@[\\]^`{|}~')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:50.867926Z",
     "iopub.status.busy": "2020-06-30T15:09:50.866936Z",
     "iopub.status.idle": "2020-06-30T15:09:50.896934Z",
     "shell.execute_reply": "2020-06-30T15:09:50.895954Z",
     "shell.execute_reply.started": "2020-06-30T15:09:50.867926Z"
    },
    "id": "yNHQtkszskxr",
    "outputId": "b5b51cbe-7bc5-4b5b-e905-9bfcd60946c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat_289': 1,\n",
       " 'cat_335': 2,\n",
       " 'cat_22': 3,\n",
       " 'cat_118': 4,\n",
       " 'cat_352': 5,\n",
       " 'cat_225': 6,\n",
       " 'cat_280': 7,\n",
       " 'cat_72': 8,\n",
       " 'cat_207': 9,\n",
       " 'cat_41': 10,\n",
       " 'cat_169': 11,\n",
       " 'cat_92': 12,\n",
       " 'cat_132': 13,\n",
       " 'cat_127': 14,\n",
       " 'cat_185': 15,\n",
       " 'cat_348': 16,\n",
       " 'cat_115': 17,\n",
       " 'cat_143': 18,\n",
       " 'cat_259': 19,\n",
       " 'cat_208': 20,\n",
       " 'cat_328': 21,\n",
       " 'cat_141': 22,\n",
       " 'cat_278': 23,\n",
       " 'cat_290': 24,\n",
       " 'cat_160': 25,\n",
       " 'cat_347': 26,\n",
       " 'cat_75': 27,\n",
       " 'cat_34': 28,\n",
       " 'cat_255': 29,\n",
       " 'cat_71': 30,\n",
       " 'cat_60': 31,\n",
       " 'cat_183': 32,\n",
       " 'cat_206': 33,\n",
       " 'cat_139': 34,\n",
       " 'cat_32': 35,\n",
       " 'cat_247': 36,\n",
       " 'cat_298': 37,\n",
       " 'cat_288': 38,\n",
       " 'cat_173': 39,\n",
       " 'cat_336': 40,\n",
       " 'cat_38': 41,\n",
       " 'cat_220': 42,\n",
       " 'cat_201': 43,\n",
       " 'cat_197': 44,\n",
       " 'cat_7': 45,\n",
       " 'cat_309': 46,\n",
       " 'cat_346': 47,\n",
       " 'cat_216': 48,\n",
       " 'cat_212': 49,\n",
       " 'cat_334': 50,\n",
       " 'cat_331': 51,\n",
       " 'cat_159': 52,\n",
       " 'cat_9': 53,\n",
       " 'cat_339': 54,\n",
       " 'cat_117': 55,\n",
       " 'cat_230': 56,\n",
       " 'cat_214': 57,\n",
       " 'cat_241': 58,\n",
       " 'cat_320': 59,\n",
       " 'cat_359': 60,\n",
       " 'cat_266': 61,\n",
       " 'cat_49': 62,\n",
       " 'cat_179': 63,\n",
       " 'cat_191': 64,\n",
       " 'cat_76': 65,\n",
       " 'cat_285': 66,\n",
       " 'cat_146': 67,\n",
       " 'cat_357': 68,\n",
       " 'cat_140': 69,\n",
       " 'cat_86': 70,\n",
       " 'cat_267': 71,\n",
       " 'cat_17': 72,\n",
       " 'cat_56': 73,\n",
       " 'cat_344': 74,\n",
       " 'cat_70': 75,\n",
       " 'cat_246': 76,\n",
       " 'cat_11': 77,\n",
       " 'cat_52': 78,\n",
       " 'cat_35': 79,\n",
       " 'cat_82': 80,\n",
       " 'cat_2': 81,\n",
       " 'cat_137': 82,\n",
       " 'cat_50': 83,\n",
       " 'cat_283': 84,\n",
       " 'cat_274': 85,\n",
       " 'cat_14': 86,\n",
       " 'cat_196': 87,\n",
       " 'cat_83': 88,\n",
       " 'cat_167': 89,\n",
       " 'cat_245': 90,\n",
       " 'cat_20': 91,\n",
       " 'cat_188': 92,\n",
       " 'cat_36': 93,\n",
       " 'cat_98': 94,\n",
       " 'cat_237': 95,\n",
       " 'cat_162': 96,\n",
       " 'cat_13': 97,\n",
       " 'cat_91': 98,\n",
       " 'cat_93': 99,\n",
       " 'cat_55': 100,\n",
       " 'cat_322': 101,\n",
       " 'cat_37': 102,\n",
       " 'cat_261': 103,\n",
       " 'cat_213': 104,\n",
       " 'cat_244': 105,\n",
       " 'cat_144': 106,\n",
       " 'cat_343': 107,\n",
       " 'cat_175': 108,\n",
       " 'cat_231': 109,\n",
       " 'cat_257': 110,\n",
       " 'cat_154': 111,\n",
       " 'cat_43': 112,\n",
       " 'cat_294': 113,\n",
       " 'cat_326': 114,\n",
       " 'cat_291': 115,\n",
       " 'cat_254': 116,\n",
       " 'cat_252': 117,\n",
       " 'cat_59': 118,\n",
       " 'cat_31': 119,\n",
       " 'cat_242': 120,\n",
       " 'cat_96': 121,\n",
       " 'cat_25': 122,\n",
       " 'cat_125': 123,\n",
       " 'cat_27': 124,\n",
       " 'cat_265': 125,\n",
       " 'cat_293': 126,\n",
       " 'cat_195': 127,\n",
       " 'cat_130': 128,\n",
       " 'cat_68': 129,\n",
       " 'cat_156': 130,\n",
       " 'cat_215': 131,\n",
       " 'cat_62': 132,\n",
       " 'cat_281': 133,\n",
       " 'cat_10': 134,\n",
       " 'cat_270': 135,\n",
       " 'cat_354': 136,\n",
       " 'cat_338': 137,\n",
       " 'cat_256': 138,\n",
       " 'cat_166': 139,\n",
       " 'cat_190': 140,\n",
       " 'cat_97': 141,\n",
       " 'cat_100': 142,\n",
       " 'cat_297': 143,\n",
       " 'cat_101': 144,\n",
       " 'cat_108': 145,\n",
       " 'cat_227': 146,\n",
       " 'cat_5': 147,\n",
       " 'cat_105': 148,\n",
       " 'cat_102': 149,\n",
       " 'cat_272': 150,\n",
       " 'cat_131': 151,\n",
       " 'cat_33': 152,\n",
       " 'cat_174': 153,\n",
       " 'cat_276': 154,\n",
       " 'cat_332': 155,\n",
       " 'cat_163': 156,\n",
       " 'cat_12': 157,\n",
       " 'intencion': 158,\n",
       " 'cat_222': 159,\n",
       " 'cat_165': 160,\n",
       " 'cat_218': 161,\n",
       " 'cat_243': 162,\n",
       " 'cat_192': 163,\n",
       " 'cat_302': 164,\n",
       " 'cat_181': 165,\n",
       " 'cat_73': 166,\n",
       " 'cat_19': 167,\n",
       " 'cat_233': 168,\n",
       " 'cat_85': 169,\n",
       " 'cat_228': 170,\n",
       " 'cat_45': 171,\n",
       " 'cat_337': 172,\n",
       " 'cat_138': 173,\n",
       " 'cat_306': 174,\n",
       " 'cat_48': 175,\n",
       " 'cat_253': 176,\n",
       " 'cat_30': 177,\n",
       " 'cat_275': 178,\n",
       " 'cat_29': 179,\n",
       " 'cat_78': 180,\n",
       " 'cat_219': 181,\n",
       " 'cat_263': 182,\n",
       " 'cat_286': 183,\n",
       " 'cat_295': 184,\n",
       " 'cat_350': 185,\n",
       " 'cat_310': 186,\n",
       " 'cat_340': 187,\n",
       " 'cat_223': 188,\n",
       " 'cat_63': 189,\n",
       " 'cat_305': 190,\n",
       " 'cat_88': 191,\n",
       " 'cat_158': 192,\n",
       " 'cat_205': 193,\n",
       " 'cat_40': 194,\n",
       " 'cat_189': 195,\n",
       " 'cat_324': 196,\n",
       " 'cat_342': 197,\n",
       " 'cat_3': 198,\n",
       " 'cat_226': 199,\n",
       " 'cat_317': 200,\n",
       " 'cat_300': 201,\n",
       " 'cat_277': 202,\n",
       " 'cat_67': 203,\n",
       " 'cat_51': 204,\n",
       " 'cat_58': 205,\n",
       " 'cat_107': 206,\n",
       " 'cat_234': 207,\n",
       " 'cat_109': 208,\n",
       " 'cat_170': 209,\n",
       " 'cat_57': 210,\n",
       " 'cat_65': 211,\n",
       " 'cat_238': 212,\n",
       " 'cat_240': 213,\n",
       " 'cat_180': 214,\n",
       " 'cat_134': 215,\n",
       " 'cat_262': 216,\n",
       " 'cat_114': 217,\n",
       " 'cat_248': 218,\n",
       " 'cat_345': 219,\n",
       " 'cat_145': 220,\n",
       " 'cat_264': 221,\n",
       " 'cat_303': 222,\n",
       " 'cat_8': 223,\n",
       " 'cat_314': 224,\n",
       " 'cat_157': 225,\n",
       " 'cat_321': 226,\n",
       " 'cat_186': 227,\n",
       " 'cat_268': 228,\n",
       " 'cat_284': 229,\n",
       " 'cat_260': 230,\n",
       " 'cat_200': 231,\n",
       " 'cat_341': 232,\n",
       " 'cat_164': 233,\n",
       " 'cat_210': 234,\n",
       " 'cat_279': 235,\n",
       " 'cat_89': 236,\n",
       " 'cat_355': 237,\n",
       " 'cat_198': 238,\n",
       " 'cat_66': 239,\n",
       " 'cat_46': 240,\n",
       " 'cat_323': 241,\n",
       " 'cat_235': 242,\n",
       " 'cat_358': 243,\n",
       " 'cat_184': 244,\n",
       " 'cat_224': 245,\n",
       " 'cat_133': 246,\n",
       " 'cat_84': 247,\n",
       " 'cat_104': 248,\n",
       " 'cat_126': 249,\n",
       " 'cat_311': 250,\n",
       " 'cat_15': 251,\n",
       " 'cat_81': 252,\n",
       " 'cat_149': 253,\n",
       " 'cat_129': 254,\n",
       " 'cat_161': 255,\n",
       " 'cat_299': 256,\n",
       " 'cat_24': 257,\n",
       " 'cat_249': 258,\n",
       " 'cat_42': 259,\n",
       " 'cat_0': 260,\n",
       " 'cat_319': 261,\n",
       " 'cat_307': 262,\n",
       " 'cat_351': 263,\n",
       " 'cat_329': 264,\n",
       " 'cat_282': 265,\n",
       " 'cat_16': 266,\n",
       " 'cat_353': 267,\n",
       " 'cat_182': 268,\n",
       " 'cat_292': 269,\n",
       " 'cat_74': 270,\n",
       " 'cat_209': 271,\n",
       " 'cat_79': 272,\n",
       " 'cat_315': 273,\n",
       " 'cat_39': 274,\n",
       " 'cat_142': 275,\n",
       " 'cat_47': 276,\n",
       " 'cat_64': 277,\n",
       " 'cat_193': 278,\n",
       " 'cat_269': 279,\n",
       " 'cat_147': 280,\n",
       " 'cat_172': 281,\n",
       " 'cat_211': 282,\n",
       " 'cat_229': 283,\n",
       " 'cat_221': 284,\n",
       " 'cat_313': 285,\n",
       " 'cat_273': 286,\n",
       " 'cat_271': 287,\n",
       " 'cat_6': 288,\n",
       " 'cat_112': 289,\n",
       " 'cat_327': 290,\n",
       " 'cat_349': 291,\n",
       " 'cat_308': 292,\n",
       " 'cat_135': 293,\n",
       " 'cat_99': 294,\n",
       " 'cat_177': 295,\n",
       " 'cat_287': 296,\n",
       " 'cat_171': 297,\n",
       " 'cat_316': 298,\n",
       " 'cat_296': 299,\n",
       " 'cat_122': 300,\n",
       " 'cat_148': 301,\n",
       " 'cat_54': 302,\n",
       " 'cat_312': 303,\n",
       " 'cat_239': 304,\n",
       " 'cat_232': 305,\n",
       " 'cat_44': 306,\n",
       " 'cat_202': 307,\n",
       " 'cat_325': 308,\n",
       " 'cat_150': 309,\n",
       " 'cat_236': 310,\n",
       " 'cat_153': 311,\n",
       " 'cat_69': 312,\n",
       " 'cat_23': 313,\n",
       " 'cat_187': 314,\n",
       " 'cat_95': 315,\n",
       " 'cat_18': 316,\n",
       " 'cat_304': 317,\n",
       " 'cat_155': 318,\n",
       " 'cat_151': 319,\n",
       " 'cat_61': 320,\n",
       " 'cat_53': 321,\n",
       " 'cat_152': 322,\n",
       " 'cat_251': 323,\n",
       " 'cat_194': 324,\n",
       " 'cat_217': 325,\n",
       " 'cat_1': 326,\n",
       " 'cat_87': 327,\n",
       " 'cat_110': 328,\n",
       " 'cat_4': 329,\n",
       " 'cat_124': 330,\n",
       " 'cat_176': 331,\n",
       " 'cat_26': 332,\n",
       " 'cat_204': 333,\n",
       " 'cat_106': 334,\n",
       " 'cat_128': 335,\n",
       " 'cat_301': 336,\n",
       " 'cat_333': 337,\n",
       " 'cat_318': 338,\n",
       " 'cat_330': 339,\n",
       " 'cat_168': 340,\n",
       " 'cat_90': 341,\n",
       " 'cat_77': 342,\n",
       " 'cat_120': 343,\n",
       " 'cat_94': 344,\n",
       " 'cat_250': 345,\n",
       " 'cat_203': 346,\n",
       " 'cat_103': 347,\n",
       " 'cat_199': 348,\n",
       " 'cat_80': 349,\n",
       " 'cat_178': 350,\n",
       " 'cat_258': 351,\n",
       " 'cat_356': 352,\n",
       " 'cat_136': 353}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:50.897922Z",
     "iopub.status.busy": "2020-06-30T15:09:50.897922Z",
     "iopub.status.idle": "2020-06-30T15:09:51.086979Z",
     "shell.execute_reply": "2020-06-30T15:09:51.086017Z",
     "shell.execute_reply.started": "2020-06-30T15:09:50.897922Z"
    },
    "id": "7OOx9qdBto1-"
   },
   "outputs": [],
   "source": [
    "encoded_output = encoding_doc(output_tokenizer, intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:51.087932Z",
     "iopub.status.busy": "2020-06-30T15:09:51.087932Z",
     "iopub.status.idle": "2020-06-30T15:09:51.102945Z",
     "shell.execute_reply": "2020-06-30T15:09:51.101930Z",
     "shell.execute_reply.started": "2020-06-30T15:09:51.087932Z"
    },
    "id": "0_5Lv5PiyG-z"
   },
   "outputs": [],
   "source": [
    "encoded_output = np.array(encoded_output).reshape(len(encoded_output), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:51.104956Z",
     "iopub.status.busy": "2020-06-30T15:09:51.104956Z",
     "iopub.status.idle": "2020-06-30T15:09:51.118925Z",
     "shell.execute_reply": "2020-06-30T15:09:51.117968Z",
     "shell.execute_reply.started": "2020-06-30T15:09:51.104956Z"
    },
    "id": "dpM86WrVQlx5",
    "outputId": "1521e9fb-f5b4-4c8b-8de7-1e965a5edbde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20105, 1)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:51.120927Z",
     "iopub.status.busy": "2020-06-30T15:09:51.120927Z",
     "iopub.status.idle": "2020-06-30T15:09:51.134925Z",
     "shell.execute_reply": "2020-06-30T15:09:51.133927Z",
     "shell.execute_reply.started": "2020-06-30T15:09:51.120927Z"
    },
    "id": "rD3QN-RPzfet"
   },
   "outputs": [],
   "source": [
    "def one_hot(encode):\n",
    "    o = OneHotEncoder(sparse = False)\n",
    "    return(o.fit_transform(encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:51.136945Z",
     "iopub.status.busy": "2020-06-30T15:09:51.136945Z",
     "iopub.status.idle": "2020-06-30T15:09:51.182922Z",
     "shell.execute_reply": "2020-06-30T15:09:51.181922Z",
     "shell.execute_reply.started": "2020-06-30T15:09:51.136945Z"
    },
    "id": "Z6wP_Xed7RNR"
   },
   "outputs": [],
   "source": [
    "output_one_hot = one_hot(encoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:51.185925Z",
     "iopub.status.busy": "2020-06-30T15:09:51.184926Z",
     "iopub.status.idle": "2020-06-30T15:09:51.198922Z",
     "shell.execute_reply": "2020-06-30T15:09:51.197941Z",
     "shell.execute_reply.started": "2020-06-30T15:09:51.185925Z"
    },
    "id": "A6HVslLTHgOM",
    "outputId": "197f7bf2-a371-45a8-e0e2-d1f473d7cacb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20105, 353)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:51.201937Z",
     "iopub.status.busy": "2020-06-30T15:09:51.200930Z",
     "iopub.status.idle": "2020-06-30T15:09:51.214933Z",
     "shell.execute_reply": "2020-06-30T15:09:51.213926Z",
     "shell.execute_reply.started": "2020-06-30T15:09:51.201937Z"
    },
    "id": "EqABUESD7xi9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:51.219927Z",
     "iopub.status.busy": "2020-06-30T15:09:51.218933Z",
     "iopub.status.idle": "2020-06-30T15:09:51.389922Z",
     "shell.execute_reply": "2020-06-30T15:09:51.388956Z",
     "shell.execute_reply.started": "2020-06-30T15:09:51.219927Z"
    },
    "id": "h8P4HTz6A4E-"
   },
   "outputs": [],
   "source": [
    "#train_X, val_X, train_Y, val_Y = train_test_split(padded_doc, output_one_hot, shuffle = True, test_size = 0.2)\n",
    "train_X, val_X, train_Y, val_Y = train_test_split(padded_doc, output_one_hot, shuffle = True, test_size = 0.2)\n",
    "stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "tokenizer = CustomTokenizer()\n",
    "tfidf_vect = TfidfVectorizer(lowercase=True, \n",
    "                             stop_words=stopwords,                              \n",
    "                             strip_accents='ascii', \n",
    "                             tokenizer=tokenizer,\n",
    "                             ngram_range= (1,2),\n",
    "                             sublinear_tf=True,\n",
    "                             analyzer='word',\n",
    "                             token_pattern=\"[\\w']+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:51.391924Z",
     "iopub.status.busy": "2020-06-30T15:09:51.390925Z",
     "iopub.status.idle": "2020-06-30T15:09:51.405923Z",
     "shell.execute_reply": "2020-06-30T15:09:51.404922Z",
     "shell.execute_reply.started": "2020-06-30T15:09:51.390925Z"
    },
    "id": "7E0uhC2OCtTx",
    "outputId": "9f3d4ea4-2c56-40db-b0fb-521857c0b069"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X = (16084, 46) and train_Y = (16084, 353)\n",
      "Shape of val_X = (4021, 46) and val_Y = (4021, 353)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of train_X = %s and train_Y = %s\" % (train_X.shape, train_Y.shape))\n",
    "print(\"Shape of val_X = %s and val_Y = %s\" % (val_X.shape, val_Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:51.407924Z",
     "iopub.status.busy": "2020-06-30T15:09:51.406937Z",
     "iopub.status.idle": "2020-06-30T15:09:51.422924Z",
     "shell.execute_reply": "2020-06-30T15:09:51.420981Z",
     "shell.execute_reply.started": "2020-06-30T15:09:51.407924Z"
    },
    "id": "e5BU_x74DNEb"
   },
   "outputs": [],
   "source": [
    "def create_model(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 128, input_length = max_length, trainable = False))\n",
    "    model.add(Bidirectional(LSTM(128)))\n",
    "#   model.add(LSTM(128))\n",
    "    model.add(Dense(32, activation = \"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(353, activation = \"softmax\"))\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:51.423926Z",
     "iopub.status.busy": "2020-06-30T15:09:51.423926Z",
     "iopub.status.idle": "2020-06-30T15:09:51.880922Z",
     "shell.execute_reply": "2020-06-30T15:09:51.879958Z",
     "shell.execute_reply.started": "2020-06-30T15:09:51.423926Z"
    },
    "id": "f-NvE0P7MFCe",
    "outputId": "ed74e659-fa77-4905-9922-64c0dc7cfd85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 46, 128)           782976    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 353)               11649     \n",
      "=================================================================\n",
      "Total params: 1,066,017\n",
      "Trainable params: 283,041\n",
      "Non-trainable params: 782,976\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocab_size, max_length)\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2020-06-30T15:09:51.881926Z",
     "iopub.status.busy": "2020-06-30T15:09:51.881926Z",
     "iopub.status.idle": "2020-06-30T16:43:49.444125Z",
     "shell.execute_reply": "2020-06-30T16:43:49.442125Z",
     "shell.execute_reply.started": "2020-06-30T15:09:51.881926Z"
    },
    "id": "_r-dxm2sMQ-d",
    "outputId": "bd8378f3-850f-404c-cd35-0988c5eaf523"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16084 samples, validate on 4021 samples\n",
      "Epoch 1/200\n",
      "16084/16084 [==============================] - 40s 3ms/step - loss: 5.3820 - accuracy: 0.0233 - val_loss: 5.1496 - val_accuracy: 0.0271\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.14964, saving model to model.h5\n",
      "Epoch 2/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 5.1627 - accuracy: 0.0321 - val_loss: 5.0696 - val_accuracy: 0.0497\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.14964 to 5.06956, saving model to model.h5\n",
      "Epoch 3/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 5.0829 - accuracy: 0.0365 - val_loss: 4.9653 - val_accuracy: 0.0490\n",
      "\n",
      "Epoch 00003: val_loss improved from 5.06956 to 4.96534, saving model to model.h5\n",
      "Epoch 4/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 4.9481 - accuracy: 0.0506 - val_loss: 4.7912 - val_accuracy: 0.0734\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.96534 to 4.79121, saving model to model.h5\n",
      "Epoch 5/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 4.8190 - accuracy: 0.0648 - val_loss: 4.6733 - val_accuracy: 0.0853\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.79121 to 4.67326, saving model to model.h5\n",
      "Epoch 6/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 4.7192 - accuracy: 0.0744 - val_loss: 4.5810 - val_accuracy: 0.1052\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.67326 to 4.58099, saving model to model.h5\n",
      "Epoch 7/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 4.6332 - accuracy: 0.0892 - val_loss: 4.4693 - val_accuracy: 0.1132\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.58099 to 4.46934, saving model to model.h5\n",
      "Epoch 8/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 4.5576 - accuracy: 0.0957 - val_loss: 4.3570 - val_accuracy: 0.1360\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.46934 to 4.35696, saving model to model.h5\n",
      "Epoch 9/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 4.4801 - accuracy: 0.1077 - val_loss: 4.3018 - val_accuracy: 0.1430\n",
      "\n",
      "Epoch 00009: val_loss improved from 4.35696 to 4.30182, saving model to model.h5\n",
      "Epoch 10/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 4.4213 - accuracy: 0.1147 - val_loss: 4.2363 - val_accuracy: 0.1495\n",
      "\n",
      "Epoch 00010: val_loss improved from 4.30182 to 4.23635, saving model to model.h5\n",
      "Epoch 11/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 4.3613 - accuracy: 0.1217 - val_loss: 4.1674 - val_accuracy: 0.1574\n",
      "\n",
      "Epoch 00011: val_loss improved from 4.23635 to 4.16740, saving model to model.h5\n",
      "Epoch 12/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 4.2924 - accuracy: 0.1344 - val_loss: 4.1205 - val_accuracy: 0.1636\n",
      "\n",
      "Epoch 00012: val_loss improved from 4.16740 to 4.12052, saving model to model.h5\n",
      "Epoch 13/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 4.2468 - accuracy: 0.1339 - val_loss: 4.0643 - val_accuracy: 0.1736\n",
      "\n",
      "Epoch 00013: val_loss improved from 4.12052 to 4.06426, saving model to model.h5\n",
      "Epoch 14/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 4.1928 - accuracy: 0.1425 - val_loss: 4.0239 - val_accuracy: 0.1756\n",
      "\n",
      "Epoch 00014: val_loss improved from 4.06426 to 4.02389, saving model to model.h5\n",
      "Epoch 15/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 4.1430 - accuracy: 0.1516 - val_loss: 3.9592 - val_accuracy: 0.1935\n",
      "\n",
      "Epoch 00015: val_loss improved from 4.02389 to 3.95919, saving model to model.h5\n",
      "Epoch 16/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 4.0885 - accuracy: 0.1609 - val_loss: 3.9156 - val_accuracy: 0.1975\n",
      "\n",
      "Epoch 00016: val_loss improved from 3.95919 to 3.91560, saving model to model.h5\n",
      "Epoch 17/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 4.0426 - accuracy: 0.1658 - val_loss: 3.8847 - val_accuracy: 0.1987\n",
      "\n",
      "Epoch 00017: val_loss improved from 3.91560 to 3.88469, saving model to model.h5\n",
      "Epoch 18/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.9925 - accuracy: 0.1690 - val_loss: 3.8448 - val_accuracy: 0.2104\n",
      "\n",
      "Epoch 00018: val_loss improved from 3.88469 to 3.84476, saving model to model.h5\n",
      "Epoch 19/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.9468 - accuracy: 0.1768 - val_loss: 3.7905 - val_accuracy: 0.2211\n",
      "\n",
      "Epoch 00019: val_loss improved from 3.84476 to 3.79048, saving model to model.h5\n",
      "Epoch 20/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.8961 - accuracy: 0.1829 - val_loss: 3.7487 - val_accuracy: 0.2221\n",
      "\n",
      "Epoch 00020: val_loss improved from 3.79048 to 3.74865, saving model to model.h5\n",
      "Epoch 21/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.8685 - accuracy: 0.1834 - val_loss: 3.7121 - val_accuracy: 0.2298\n",
      "\n",
      "Epoch 00021: val_loss improved from 3.74865 to 3.71211, saving model to model.h5\n",
      "Epoch 22/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.8287 - accuracy: 0.1889 - val_loss: 3.6944 - val_accuracy: 0.2358\n",
      "\n",
      "Epoch 00022: val_loss improved from 3.71211 to 3.69440, saving model to model.h5\n",
      "Epoch 23/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.7966 - accuracy: 0.1927 - val_loss: 3.6452 - val_accuracy: 0.2303\n",
      "\n",
      "Epoch 00023: val_loss improved from 3.69440 to 3.64518, saving model to model.h5\n",
      "Epoch 24/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.7554 - accuracy: 0.1978 - val_loss: 3.6068 - val_accuracy: 0.2487\n",
      "\n",
      "Epoch 00024: val_loss improved from 3.64518 to 3.60683, saving model to model.h5\n",
      "Epoch 25/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.6992 - accuracy: 0.2082 - val_loss: 3.5701 - val_accuracy: 0.2539\n",
      "\n",
      "Epoch 00025: val_loss improved from 3.60683 to 3.57008, saving model to model.h5\n",
      "Epoch 26/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.6647 - accuracy: 0.2121 - val_loss: 3.5281 - val_accuracy: 0.2609\n",
      "\n",
      "Epoch 00026: val_loss improved from 3.57008 to 3.52813, saving model to model.h5\n",
      "Epoch 27/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.6113 - accuracy: 0.2156 - val_loss: 3.4951 - val_accuracy: 0.2624\n",
      "\n",
      "Epoch 00027: val_loss improved from 3.52813 to 3.49509, saving model to model.h5\n",
      "Epoch 28/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.5728 - accuracy: 0.2244 - val_loss: 3.4841 - val_accuracy: 0.2678\n",
      "\n",
      "Epoch 00028: val_loss improved from 3.49509 to 3.48405, saving model to model.h5\n",
      "Epoch 29/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.5443 - accuracy: 0.2267 - val_loss: 3.4532 - val_accuracy: 0.2718\n",
      "\n",
      "Epoch 00029: val_loss improved from 3.48405 to 3.45321, saving model to model.h5\n",
      "Epoch 30/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.5014 - accuracy: 0.2349 - val_loss: 3.4286 - val_accuracy: 0.2763\n",
      "\n",
      "Epoch 00030: val_loss improved from 3.45321 to 3.42859, saving model to model.h5\n",
      "Epoch 31/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.4706 - accuracy: 0.2390 - val_loss: 3.4076 - val_accuracy: 0.2825\n",
      "\n",
      "Epoch 00031: val_loss improved from 3.42859 to 3.40761, saving model to model.h5\n",
      "Epoch 32/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.4449 - accuracy: 0.2415 - val_loss: 3.3837 - val_accuracy: 0.2848\n",
      "\n",
      "Epoch 00032: val_loss improved from 3.40761 to 3.38373, saving model to model.h5\n",
      "Epoch 33/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.4131 - accuracy: 0.2417 - val_loss: 3.3615 - val_accuracy: 0.2862\n",
      "\n",
      "Epoch 00033: val_loss improved from 3.38373 to 3.36154, saving model to model.h5\n",
      "Epoch 34/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.3831 - accuracy: 0.2498 - val_loss: 3.3477 - val_accuracy: 0.2850\n",
      "\n",
      "Epoch 00034: val_loss improved from 3.36154 to 3.34775, saving model to model.h5\n",
      "Epoch 35/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.3430 - accuracy: 0.2535 - val_loss: 3.3585 - val_accuracy: 0.2922\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 3.34775\n",
      "Epoch 36/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.3237 - accuracy: 0.2581 - val_loss: 3.3333 - val_accuracy: 0.2932\n",
      "\n",
      "Epoch 00036: val_loss improved from 3.34775 to 3.33333, saving model to model.h5\n",
      "Epoch 37/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.2666 - accuracy: 0.2634 - val_loss: 3.3154 - val_accuracy: 0.3044\n",
      "\n",
      "Epoch 00037: val_loss improved from 3.33333 to 3.31537, saving model to model.h5\n",
      "Epoch 38/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.2458 - accuracy: 0.2699 - val_loss: 3.2897 - val_accuracy: 0.3032\n",
      "\n",
      "Epoch 00038: val_loss improved from 3.31537 to 3.28967, saving model to model.h5\n",
      "Epoch 39/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.2083 - accuracy: 0.2707 - val_loss: 3.2507 - val_accuracy: 0.3139\n",
      "\n",
      "Epoch 00039: val_loss improved from 3.28967 to 3.25073, saving model to model.h5\n",
      "Epoch 40/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.1631 - accuracy: 0.2816 - val_loss: 3.2576 - val_accuracy: 0.3156\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 3.25073\n",
      "Epoch 41/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.1327 - accuracy: 0.2803 - val_loss: 3.2388 - val_accuracy: 0.3228\n",
      "\n",
      "Epoch 00041: val_loss improved from 3.25073 to 3.23882, saving model to model.h5\n",
      "Epoch 42/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.1117 - accuracy: 0.2898 - val_loss: 3.2224 - val_accuracy: 0.3226\n",
      "\n",
      "Epoch 00042: val_loss improved from 3.23882 to 3.22240, saving model to model.h5\n",
      "Epoch 43/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.0923 - accuracy: 0.2901 - val_loss: 3.2052 - val_accuracy: 0.3305\n",
      "\n",
      "Epoch 00043: val_loss improved from 3.22240 to 3.20524, saving model to model.h5\n",
      "Epoch 44/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 3.0669 - accuracy: 0.2887 - val_loss: 3.2276 - val_accuracy: 0.3283\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 3.20524\n",
      "Epoch 45/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 3.0472 - accuracy: 0.2960 - val_loss: 3.1730 - val_accuracy: 0.3357\n",
      "\n",
      "Epoch 00045: val_loss improved from 3.20524 to 3.17298, saving model to model.h5\n",
      "Epoch 46/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 3.0257 - accuracy: 0.3005 - val_loss: 3.1972 - val_accuracy: 0.3380\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 3.17298\n",
      "Epoch 47/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.9958 - accuracy: 0.3001 - val_loss: 3.1579 - val_accuracy: 0.3464\n",
      "\n",
      "Epoch 00047: val_loss improved from 3.17298 to 3.15791, saving model to model.h5\n",
      "Epoch 48/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.9550 - accuracy: 0.3088 - val_loss: 3.1356 - val_accuracy: 0.3442\n",
      "\n",
      "Epoch 00048: val_loss improved from 3.15791 to 3.13560, saving model to model.h5\n",
      "Epoch 49/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 2.9412 - accuracy: 0.3109 - val_loss: 3.1862 - val_accuracy: 0.3454\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 3.13560\n",
      "Epoch 50/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.9219 - accuracy: 0.3176 - val_loss: 3.1364 - val_accuracy: 0.3482\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 3.13560\n",
      "Epoch 51/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.9021 - accuracy: 0.3137 - val_loss: 3.1660 - val_accuracy: 0.3474\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 3.13560\n",
      "Epoch 52/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.8596 - accuracy: 0.3242 - val_loss: 3.1529 - val_accuracy: 0.3479\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 3.13560\n",
      "Epoch 53/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 2.8442 - accuracy: 0.3232 - val_loss: 3.1689 - val_accuracy: 0.3549\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 3.13560\n",
      "Epoch 54/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.8281 - accuracy: 0.3312 - val_loss: 3.1633 - val_accuracy: 0.3534\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 3.13560\n",
      "Epoch 55/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.8571 - accuracy: 0.3224 - val_loss: 3.1504 - val_accuracy: 0.3561\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 3.13560\n",
      "Epoch 56/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.7767 - accuracy: 0.3360 - val_loss: 3.1288 - val_accuracy: 0.3638\n",
      "\n",
      "Epoch 00056: val_loss improved from 3.13560 to 3.12884, saving model to model.h5\n",
      "Epoch 57/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 2.7476 - accuracy: 0.3408 - val_loss: 3.1674 - val_accuracy: 0.3693\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 3.12884\n",
      "Epoch 58/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 2.7458 - accuracy: 0.3410 - val_loss: 3.1126 - val_accuracy: 0.3641\n",
      "\n",
      "Epoch 00058: val_loss improved from 3.12884 to 3.11260, saving model to model.h5\n",
      "Epoch 59/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.7402 - accuracy: 0.3466 - val_loss: 3.1345 - val_accuracy: 0.3678\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 3.11260\n",
      "Epoch 60/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 2.6946 - accuracy: 0.3508 - val_loss: 3.1278 - val_accuracy: 0.3760\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 3.11260\n",
      "Epoch 61/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.6697 - accuracy: 0.3567 - val_loss: 3.0895 - val_accuracy: 0.3711\n",
      "\n",
      "Epoch 00061: val_loss improved from 3.11260 to 3.08947, saving model to model.h5\n",
      "Epoch 62/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.6549 - accuracy: 0.3558 - val_loss: 3.1634 - val_accuracy: 0.3725\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 3.08947\n",
      "Epoch 63/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.6568 - accuracy: 0.3589 - val_loss: 3.1191 - val_accuracy: 0.3770\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 3.08947\n",
      "Epoch 64/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 2.6216 - accuracy: 0.3571 - val_loss: 3.1241 - val_accuracy: 0.3850\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 3.08947\n",
      "Epoch 65/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.6056 - accuracy: 0.3638 - val_loss: 3.0980 - val_accuracy: 0.3865\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 3.08947\n",
      "Epoch 66/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 2.5734 - accuracy: 0.3683 - val_loss: 3.1470 - val_accuracy: 0.3850\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 3.08947\n",
      "Epoch 67/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 2.6163 - accuracy: 0.3651 - val_loss: 3.1003 - val_accuracy: 0.3852\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 3.08947\n",
      "Epoch 68/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 2.5971 - accuracy: 0.3657 - val_loss: 3.1028 - val_accuracy: 0.3882\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 3.08947\n",
      "Epoch 69/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.5315 - accuracy: 0.3763 - val_loss: 3.1486 - val_accuracy: 0.3934\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 3.08947\n",
      "Epoch 70/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.5059 - accuracy: 0.3819 - val_loss: 3.1622 - val_accuracy: 0.3929\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 3.08947\n",
      "Epoch 71/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.4807 - accuracy: 0.3854 - val_loss: 3.1841 - val_accuracy: 0.3982\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 3.08947\n",
      "Epoch 72/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.4725 - accuracy: 0.3844 - val_loss: 3.1112 - val_accuracy: 0.3984\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 3.08947\n",
      "Epoch 73/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 2.4755 - accuracy: 0.3866 - val_loss: 3.1539 - val_accuracy: 0.3974\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 3.08947\n",
      "Epoch 74/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.4347 - accuracy: 0.3926 - val_loss: 3.1883 - val_accuracy: 0.3967\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 3.08947\n",
      "Epoch 75/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.4273 - accuracy: 0.3916 - val_loss: 3.1866 - val_accuracy: 0.3974\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 3.08947\n",
      "Epoch 76/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.4169 - accuracy: 0.3927 - val_loss: 3.2139 - val_accuracy: 0.4004\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 3.08947\n",
      "Epoch 77/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.4000 - accuracy: 0.3974 - val_loss: 3.2163 - val_accuracy: 0.4006\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 3.08947\n",
      "Epoch 78/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.3753 - accuracy: 0.4020 - val_loss: 3.1944 - val_accuracy: 0.4074\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 3.08947\n",
      "Epoch 79/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.3631 - accuracy: 0.4039 - val_loss: 3.2433 - val_accuracy: 0.4069\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 3.08947\n",
      "Epoch 80/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.3519 - accuracy: 0.4044 - val_loss: 3.2605 - val_accuracy: 0.4133\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 3.08947\n",
      "Epoch 81/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 2.3477 - accuracy: 0.4057 - val_loss: 3.2244 - val_accuracy: 0.4086\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 3.08947\n",
      "Epoch 82/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.3534 - accuracy: 0.4051 - val_loss: 3.2048 - val_accuracy: 0.4126\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 3.08947\n",
      "Epoch 83/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.3119 - accuracy: 0.4126 - val_loss: 3.2388 - val_accuracy: 0.4151\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 3.08947\n",
      "Epoch 84/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 2.3500 - accuracy: 0.4097 - val_loss: 3.1966 - val_accuracy: 0.4031\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 3.08947\n",
      "Epoch 85/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.3382 - accuracy: 0.4066 - val_loss: 3.2247 - val_accuracy: 0.4131\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 3.08947\n",
      "Epoch 86/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 2.2892 - accuracy: 0.4138 - val_loss: 3.2572 - val_accuracy: 0.4123\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 3.08947\n",
      "Epoch 87/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.2598 - accuracy: 0.4186 - val_loss: 3.2513 - val_accuracy: 0.4163\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 3.08947\n",
      "Epoch 88/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 2.2406 - accuracy: 0.4228 - val_loss: 3.2915 - val_accuracy: 0.4243\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 3.08947\n",
      "Epoch 89/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.2155 - accuracy: 0.4348 - val_loss: 3.3236 - val_accuracy: 0.4148\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 3.08947\n",
      "Epoch 90/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.2201 - accuracy: 0.4334 - val_loss: 3.3550 - val_accuracy: 0.4176\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 3.08947\n",
      "Epoch 91/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.2080 - accuracy: 0.4304 - val_loss: 3.3545 - val_accuracy: 0.4190\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 3.08947\n",
      "Epoch 92/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.1870 - accuracy: 0.4322 - val_loss: 3.4105 - val_accuracy: 0.4200\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 3.08947\n",
      "Epoch 93/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 2.1597 - accuracy: 0.4405 - val_loss: 3.3451 - val_accuracy: 0.4225\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 3.08947\n",
      "Epoch 94/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 2.1534 - accuracy: 0.4389 - val_loss: 3.4011 - val_accuracy: 0.4290\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 3.08947\n",
      "Epoch 95/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 2.1492 - accuracy: 0.4368 - val_loss: 3.3660 - val_accuracy: 0.4295\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 3.08947\n",
      "Epoch 96/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.1358 - accuracy: 0.4434 - val_loss: 3.4211 - val_accuracy: 0.4307\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 3.08947\n",
      "Epoch 97/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 2.1852 - accuracy: 0.4310 - val_loss: 3.4663 - val_accuracy: 0.4283\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 3.08947\n",
      "Epoch 98/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 2.1276 - accuracy: 0.4405 - val_loss: 3.4158 - val_accuracy: 0.4278\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 3.08947\n",
      "Epoch 99/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.0819 - accuracy: 0.4549 - val_loss: 3.4866 - val_accuracy: 0.4258\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 3.08947\n",
      "Epoch 100/200\n",
      "16084/16084 [==============================] - 38s 2ms/step - loss: 2.0908 - accuracy: 0.4488 - val_loss: 3.4024 - val_accuracy: 0.4273\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 3.08947\n",
      "Epoch 101/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 2.0646 - accuracy: 0.4559 - val_loss: 3.5191 - val_accuracy: 0.4248\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 3.08947\n",
      "Epoch 102/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 2.1053 - accuracy: 0.4497 - val_loss: 3.6024 - val_accuracy: 0.4320\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 3.08947\n",
      "Epoch 103/200\n",
      "16084/16084 [==============================] - 40s 2ms/step - loss: 2.0805 - accuracy: 0.4541 - val_loss: 3.5039 - val_accuracy: 0.4350\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 3.08947\n",
      "Epoch 104/200\n",
      "16084/16084 [==============================] - 40s 2ms/step - loss: 2.0437 - accuracy: 0.4590 - val_loss: 3.4576 - val_accuracy: 0.4340\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 3.08947\n",
      "Epoch 105/200\n",
      "16084/16084 [==============================] - 40s 2ms/step - loss: 2.0211 - accuracy: 0.4598 - val_loss: 3.4453 - val_accuracy: 0.4342\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 3.08947\n",
      "Epoch 106/200\n",
      "16084/16084 [==============================] - 40s 2ms/step - loss: 2.0203 - accuracy: 0.4616 - val_loss: 3.5601 - val_accuracy: 0.4404\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 3.08947\n",
      "Epoch 107/200\n",
      "16084/16084 [==============================] - 40s 2ms/step - loss: 1.9742 - accuracy: 0.4711 - val_loss: 3.6654 - val_accuracy: 0.4382\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 3.08947\n",
      "Epoch 108/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 2.1410 - accuracy: 0.4467 - val_loss: 3.2971 - val_accuracy: 0.4260\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 3.08947\n",
      "Epoch 109/200\n",
      "16084/16084 [==============================] - 40s 3ms/step - loss: 2.0208 - accuracy: 0.4621 - val_loss: 3.4759 - val_accuracy: 0.4429\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 3.08947\n",
      "Epoch 110/200\n",
      "16084/16084 [==============================] - 40s 2ms/step - loss: 1.9685 - accuracy: 0.4764 - val_loss: 3.5992 - val_accuracy: 0.4409\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 3.08947\n",
      "Epoch 111/200\n",
      "16084/16084 [==============================] - 40s 2ms/step - loss: 1.9387 - accuracy: 0.4788 - val_loss: 3.6491 - val_accuracy: 0.4404\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 3.08947\n",
      "Epoch 112/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 1.9443 - accuracy: 0.4774 - val_loss: 3.6774 - val_accuracy: 0.4417\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 3.08947\n",
      "Epoch 113/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 1.9240 - accuracy: 0.4802 - val_loss: 3.7564 - val_accuracy: 0.4469\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 3.08947\n",
      "Epoch 114/200\n",
      "16084/16084 [==============================] - 40s 2ms/step - loss: 1.9248 - accuracy: 0.4804 - val_loss: 3.6814 - val_accuracy: 0.4327\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 3.08947\n",
      "Epoch 115/200\n",
      "16084/16084 [==============================] - 40s 2ms/step - loss: 1.9232 - accuracy: 0.4787 - val_loss: 3.6780 - val_accuracy: 0.4407\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 3.08947\n",
      "Epoch 116/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 1.9237 - accuracy: 0.4816 - val_loss: 3.6888 - val_accuracy: 0.4424\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 3.08947\n",
      "Epoch 117/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 1.8853 - accuracy: 0.4891 - val_loss: 3.6210 - val_accuracy: 0.4484\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 3.08947\n",
      "Epoch 118/200\n",
      "16084/16084 [==============================] - 40s 2ms/step - loss: 1.9388 - accuracy: 0.4779 - val_loss: 3.8423 - val_accuracy: 0.4434\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 3.08947\n",
      "Epoch 119/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 1.8860 - accuracy: 0.4909 - val_loss: 3.6309 - val_accuracy: 0.4459\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 3.08947\n",
      "Epoch 120/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 1.8892 - accuracy: 0.4848 - val_loss: 3.8049 - val_accuracy: 0.4419\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 3.08947\n",
      "Epoch 121/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 1.9192 - accuracy: 0.4841 - val_loss: 3.7884 - val_accuracy: 0.4404\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 3.08947\n",
      "Epoch 122/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 1.8862 - accuracy: 0.4933 - val_loss: 3.8175 - val_accuracy: 0.4511\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 3.08947\n",
      "Epoch 123/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 1.8513 - accuracy: 0.4969 - val_loss: 3.8937 - val_accuracy: 0.4479\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 3.08947\n",
      "Epoch 124/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 1.8952 - accuracy: 0.4889 - val_loss: 3.7302 - val_accuracy: 0.4561\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 3.08947\n",
      "Epoch 125/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 1.8365 - accuracy: 0.5023 - val_loss: 3.9139 - val_accuracy: 0.4486\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 3.08947\n",
      "Epoch 126/200\n",
      "16084/16084 [==============================] - 40s 2ms/step - loss: 1.8124 - accuracy: 0.5022 - val_loss: 3.9431 - val_accuracy: 0.4616\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 3.08947\n",
      "Epoch 127/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 1.8024 - accuracy: 0.5038 - val_loss: 3.9328 - val_accuracy: 0.4434\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 3.08947\n",
      "Epoch 128/200\n",
      "16084/16084 [==============================] - 40s 2ms/step - loss: 1.8384 - accuracy: 0.4965 - val_loss: 3.9571 - val_accuracy: 0.4489\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 3.08947\n",
      "Epoch 129/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 1.7870 - accuracy: 0.5047 - val_loss: 4.0298 - val_accuracy: 0.4549\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 3.08947\n",
      "Epoch 130/200\n",
      "16084/16084 [==============================] - 40s 2ms/step - loss: 1.7621 - accuracy: 0.5131 - val_loss: 4.0117 - val_accuracy: 0.4561\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 3.08947\n",
      "Epoch 131/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 1.7717 - accuracy: 0.5095 - val_loss: 4.0578 - val_accuracy: 0.4506\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 3.08947\n",
      "Epoch 132/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 1.7633 - accuracy: 0.5086 - val_loss: 4.0571 - val_accuracy: 0.4583\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 3.08947\n",
      "Epoch 133/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 1.7742 - accuracy: 0.5130 - val_loss: 4.1482 - val_accuracy: 0.4382\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 3.08947\n",
      "Epoch 134/200\n",
      "16084/16084 [==============================] - 40s 2ms/step - loss: 1.8589 - accuracy: 0.4954 - val_loss: 3.9539 - val_accuracy: 0.4499\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 3.08947\n",
      "Epoch 135/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 1.7657 - accuracy: 0.5125 - val_loss: 3.9047 - val_accuracy: 0.4494\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 3.08947\n",
      "Epoch 136/200\n",
      "16084/16084 [==============================] - 40s 2ms/step - loss: 1.7773 - accuracy: 0.5137 - val_loss: 4.2468 - val_accuracy: 0.4534\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 3.08947\n",
      "Epoch 137/200\n",
      "16084/16084 [==============================] - 40s 2ms/step - loss: 1.7556 - accuracy: 0.5185 - val_loss: 4.1158 - val_accuracy: 0.4514\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 3.08947\n",
      "Epoch 138/200\n",
      "16084/16084 [==============================] - 40s 2ms/step - loss: 1.7392 - accuracy: 0.5170 - val_loss: 4.0378 - val_accuracy: 0.4511\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 3.08947\n",
      "Epoch 139/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 1.7071 - accuracy: 0.5280 - val_loss: 4.2001 - val_accuracy: 0.4566\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 3.08947\n",
      "Epoch 140/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 1.6874 - accuracy: 0.5300 - val_loss: 4.3097 - val_accuracy: 0.4504\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 3.08947\n",
      "Epoch 141/200\n",
      "16084/16084 [==============================] - 40s 2ms/step - loss: 1.6988 - accuracy: 0.5253 - val_loss: 4.2143 - val_accuracy: 0.4586\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 3.08947\n",
      "Epoch 142/200\n",
      "16084/16084 [==============================] - 39s 2ms/step - loss: 1.6932 - accuracy: 0.5262 - val_loss: 4.1255 - val_accuracy: 0.4588\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 3.08947\n",
      "Epoch 143/200\n",
      "16084/16084 [==============================] - 40s 2ms/step - loss: 1.6967 - accuracy: 0.5249 - val_loss: 4.1839 - val_accuracy: 0.4581\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 3.08947\n",
      "Epoch 144/200\n",
      "16084/16084 [==============================] - 40s 2ms/step - loss: 1.6765 - accuracy: 0.5313 - val_loss: 4.1884 - val_accuracy: 0.4511\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 3.08947\n",
      "Epoch 145/200\n",
      "13248/16084 [=======================>......] - ETA: 6s - loss: 1.6962 - accuracy: 0.5263"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-7952eefcd344>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#train_X = tfidf_vect.fit_transform(train_X)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#train_Y = tfidf_vect.transform(train_Y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ds\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ds\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ds\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "#train_X = tfidf_vect.fit_transform(train_X)\n",
    "#train_Y = tfidf_vect.transform(train_Y)\n",
    "hist = model.fit(train_X, train_Y, epochs = 200, batch_size = 64, validation_data = (val_X, val_Y), callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.status.busy": "2020-06-30T16:43:49.445125Z",
     "iopub.status.idle": "2020-06-30T16:43:49.446125Z"
    },
    "id": "YjXKos8ocXvw"
   },
   "outputs": [],
   "source": [
    " model = load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.status.busy": "2020-06-30T16:43:49.447129Z",
     "iopub.status.idle": "2020-06-30T16:43:49.448128Z"
    },
    "id": "qSTEzrlzcuya"
   },
   "outputs": [],
   "source": [
    "def predictions(text):\n",
    "    clean = re.sub(r'[^ a-z A-Z 0-9]', \" \", text)\n",
    "    test_word = word_tokenize(clean)\n",
    "    test_word = [w.lower() for w in test_word]\n",
    "    test_ls = word_tokenizer.texts_to_sequences(test_word)\n",
    "    print(test_word)\n",
    "    #Check for unknown words\n",
    "    if [] in test_ls:\n",
    "        test_ls = list(filter(None, test_ls))\n",
    "      \n",
    "    test_ls = np.array(test_ls).reshape(1, len(test_ls))\n",
    "    \n",
    "    x = padding_doc(test_ls, max_length)\n",
    "    \n",
    "    pred = model.predict_proba(x)\n",
    "    \n",
    "    \n",
    "    return pred\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "execution": {
     "iopub.status.busy": "2020-06-30T16:43:49.449125Z",
     "iopub.status.idle": "2020-06-30T16:43:49.450126Z"
    },
    "id": "P1ddofshmdzK"
   },
   "outputs": [],
   "source": [
    "def get_final_output(pred, classes):\n",
    "    predictions = pred[0]\n",
    "    \n",
    "    classes = np.array(classes)\n",
    "    ids = np.argsort(-predictions)\n",
    "    classes = classes[ids]\n",
    "    predictions = -np.sort(-predictions)\n",
    "    \n",
    "    for i in range(pred.shape[1]):\n",
    "        print(\"%s has confidence = %s\" % (classes[i], (predictions[i])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "execution": {
     "iopub.status.busy": "2020-06-30T16:43:49.452127Z",
     "iopub.status.idle": "2020-06-30T16:43:49.452127Z"
    },
    "id": "23VpGuihMdEU",
    "outputId": "ba6c8f9c-894c-453d-b9f7-00aec0725e8d"
   },
   "outputs": [],
   "source": [
    "text = \"Can you help me?\"\n",
    "pred = predictions(text)\n",
    "get_final_output(pred, unique_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bKUBDT36IHKO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copia de Intent_classification_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
