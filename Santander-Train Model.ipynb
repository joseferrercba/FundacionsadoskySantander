{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VO1VDc-Nse7o"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-07-11T13:14:18.268344Z",
     "iopub.status.busy": "2020-07-11T13:14:18.268344Z",
     "iopub.status.idle": "2020-07-11T13:14:18.283359Z",
     "shell.execute_reply": "2020-07-11T13:14:18.282375Z",
     "shell.execute_reply.started": "2020-07-11T13:14:18.268344Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "C:\\ProgramData\\Anaconda3\\envs\\env-fsadosky\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import enum\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import nltk\n",
    "import mlflow\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from keras.layers import Dense, Conv2D, Embedding, Dropout, Conv1D, GlobalMaxPooling1D, LSTM, Bidirectional\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.models import Sequential, load_model\n",
    "# from keras import layers, Input, Model\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from googletrans import Translator\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import *\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from classes.Accuracy import Accuracy\n",
    "from classes.CustomTokenizer import *\n",
    "from classes.VectEnum import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Model constants.\n",
    "# max_features = 20000\n",
    "# embedding_dim = 128\n",
    "# sequence_length = 500\n",
    "# vocab_size = 30982\n",
    "# max_length = 5890#len(tfidf_vect.get_feature_names())\n",
    "# \n",
    "# # define baseline model\n",
    "# def baseline_model():\n",
    "#     # create model\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(vocab_size, 128, trainable = False))\n",
    "#     model.add(Bidirectional(LSTM(128)))\n",
    "# #   model.add(LSTM(128))\n",
    "#     model.add(Dense(32, activation = \"relu\"))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(352, activation = \"softmax\"))\n",
    "#     model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #--------------------------------------------------#\n",
    "# ### TO USE STACKING OR VOTING UNCOMMENT ALL THIS ###\n",
    "# #--------------------------------------------------#\n",
    "# # define the base models\n",
    "# level0 = list()\t\n",
    "# level0.append(('knc', KNeighborsClassifier()))\n",
    "# level0.append(('lsvc', LinearSVC()))\n",
    "# level0.append(('svm', SVC(C=10, gamma=0.1)))\t\n",
    "# # define meta learner model\n",
    "# level1 = LogisticRegression()\n",
    "# # define the stacking ensemble\n",
    "# classifier = StackingClassifier(estimators=level0, final_estimator=level1, verbose=2)\n",
    "# #classifier = VotingClassifier(estimators=level0, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #--------------------------------------------------#\n",
    "# ###                  PARAMETERS                  ###\n",
    "# #--------------------------------------------------#\n",
    "\n",
    "APPLY_RESAMPLE = False\n",
    "VECTORIZER_TYPE = VectEnum.TfidfVectorizer\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "MIN_DF = 1            \n",
    "TOKENIZER_TYPE = None #custom_preprocess | nltk.word_tokenize\n",
    "SHUFFLE = True\n",
    "SAMPLING_STRATEGY = 'not majority'\n",
    "N_JOBS = -1\n",
    "K_NEIGHBORS = 3\n",
    "SCORING = 'balanced_accuracy'\n",
    "REFIT = 'balanced_accuracy'\n",
    "CV = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-07-11T13:14:18.285348Z",
     "iopub.status.busy": "2020-07-11T13:14:18.285348Z",
     "iopub.status.idle": "2020-07-11T13:14:18.300364Z",
     "shell.execute_reply": "2020-07-11T13:14:18.298435Z",
     "shell.execute_reply.started": "2020-07-11T13:14:18.285348Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #--------------------------------------------------#\n",
    "# ###                  CLASSIFIERS                 ###\n",
    "# #--------------------------------------------------#\n",
    "\n",
    "classifier_list = []\n",
    "classifier_list.append(xgb.XGBClassifier(n_jobs=-1, verbose=2)) \n",
    "classifier_list.append(SVC(verbose=2)) \n",
    "classifier_list.append(LinearSVC(verbose=2, random_state=42)) \n",
    "classifier_list.append(RandomForestClassifier(n_jobs=-1, verbose=2)) \n",
    "classifier_list.append(BalancedRandomForestClassifier(n_jobs=-1, verbose=2)) \n",
    "classifier_list.append(KNeighborsClassifier(n_jobs=-1)) \n",
    "classifier_list.append(lgb.LGBMClassifier()) \n",
    "classifier_list.append(DecisionTreeClassifier()) \n",
    "classifier_list.append(LogisticRegression(class_weight='balanced', n_jobs=-1))\n",
    "classifier_list.append(BalancedRandomForestClassifier())\n",
    "\n",
    "oversample = SMOTE(random_state=RANDOM_STATE, n_jobs=N_JOBS, sampling_strategy=SAMPLING_STRATEGY, k_neighbors=K_NEIGHBORS)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_mock_data(df, category, multiplier=10):    \n",
    "    mask = (df['Intencion_cat_label'] == category)\n",
    "    row = df[mask]\n",
    "    return df.append([row]*multiplier,ignore_index=True)\n",
    "\n",
    "def apply_resample(df, column, num_cases=5, multiplier=10):\n",
    "    print('-----------------')\n",
    "    print('Before Resample: ')\n",
    "    print('-----------------')\n",
    "    print('Train Shape: ' + str(df.shape))\n",
    "    \n",
    "    #add another row to minority class with few values\n",
    "    grouped = df.groupby('Intencion_cat_label').count().sort_values(by='Intencion_cat_label', ascending=True)\n",
    "    poor_cases = grouped[grouped[column] < num_cases]\n",
    "    print('Poor cases: {}'.format(len(poor_cases)))\n",
    "    print(poor_cases)\n",
    "    for index in poor_cases.index.unique():\n",
    "        df = add_mock_data(df, index, multiplier)\n",
    "\n",
    "    print('----------------')\n",
    "    print('After Resample: ')\n",
    "    print('----------------')\n",
    "    print('Train Shape: ' + str(df.shape))\n",
    "    return df\n",
    "\n",
    "def __Print(text):\n",
    "    print('')\n",
    "    print('--------------------------------------------------------')\n",
    "    print('-- {} --'.format(text))\n",
    "    print('--------------------------------------------------------')        \n",
    "\n",
    "\n",
    "def GetVectorizer():        \n",
    "    if VECTORIZER_TYPE.name == 'TfidfVectorizer':\n",
    "        return TfidfVectorizer(min_df=MIN_DF, tokenizer=TOKENIZER_TYPE)\n",
    "    elif VECTORIZER_TYPE.name == 'CountVectorizer':\n",
    "        return CountVectorizer(min_df=MIN_DF, tokenizer=TOKENIZER_TYPE)            \n",
    "\n",
    "def GetPipeline():\n",
    "    vect = GetVectorizer()\n",
    "    imbpipe = imbPipeline(steps=[('vect', vect), ('resample', oversample), ('clf', classifier)], verbose=2)\n",
    "    pipeline = Pipeline(steps=[('vect', vect), ('clf', classifier)], verbose=2)\n",
    "    return imbpipe if (APPLY_RESAMPLE == True) else pipeline\n",
    "\n",
    "def Summary(model, model_name, X_train, X_test, y_train, y_test):        \n",
    "    __Print('Summary')\n",
    "    training_score = model.score(X_train , y_train)\n",
    "    test_score = model.score(X_test  , y_test )\n",
    "    mlflow.log_metric('training_score', training_score)\n",
    "    mlflow.log_metric('test_score', test_score)\n",
    "    print(\"Training set score for \" + model_name + \" %f\" % training_score)\n",
    "    print(\"Testing  set score for \" + model_name + \" %f\" % test_score)\n",
    "    print('--------------------------------------------------------')        \n",
    "\n",
    "def SaveBestParamsToDisk(model_name, model_best_params):                \n",
    "    print(\"Best Params for \" + model_name + \": \" + str(model_best_params))\n",
    "    filename = ''.join(['model_best_params/', model_name, '_best_params.json'])\n",
    "    __Print('Saving Best Parameters for {} on {}'.format(model_name, filename))\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(model_best_params, outfile)        \n",
    "\n",
    "def GetModelParams(model_name):\n",
    "    filename = 'model_params/{}_params.json'.format(model_name)\n",
    "    fileexists = os.path.isfile(filename)\n",
    "    if fileexists == False:\n",
    "        with io.open(filename, 'w') as json_file:\n",
    "            json_file.write(json.dumps({}))\n",
    "    with open(filename) as json_file:\n",
    "        params_grid = json.load(json_file)\n",
    "    return params_grid    \n",
    "\n",
    "def GenerateTrainedModel(classifier, X_train, X_test, y_train, y_test):\n",
    "    model_name = classifier.__class__.__name__\n",
    "    params_grid = GetModelParams(model_name)                                    \n",
    "    print('Preprocessing data...')        \n",
    "    pipeline = GetPipeline()         \n",
    "    gridsearch = GridSearchCV(pipeline, params_grid, cv=CV, n_jobs=N_JOBS, verbose=2, scoring=SCORING, refit=REFIT)\n",
    "    print('Training Model... {}'.format(model_name))\n",
    "    gridsearch.fit(X_train, y_train)                \n",
    "    optimized_model = gridsearch.best_estimator_\n",
    "    print('Finished Training Model.')        \n",
    "    model_best_params = gridsearch.best_params_                \n",
    "    SaveBestParamsToDisk(model_name, model_best_params)\n",
    "    Summary(optimized_model, model_name, X_train, X_test, y_train, y_test)        \n",
    "    return optimized_model, model_best_params, X_train, X_test, y_train, y_test\n",
    "\n",
    "def TrainModel(X, y, classifier):    \n",
    "    pipeline = GetPipeline()\n",
    "    print('Training Model With all train dataset...')\n",
    "    pipeline.fit(X, y)\n",
    "    print('Finished Training Model With all train dataset.')\n",
    "    __Print('Summary')\n",
    "    training_score = pipeline.score(X , y)\n",
    "    print(\"Training score for all data: %f\" % training_score)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Info from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-07-11T13:14:18.317153Z",
     "iopub.status.busy": "2020-07-11T13:14:18.317153Z",
     "iopub.status.idle": "2020-07-11T13:14:18.395149Z",
     "shell.execute_reply": "2020-07-11T13:14:18.394174Z",
     "shell.execute_reply.started": "2020-07-11T13:14:18.317153Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = shuffle(pd.read_csv('data/train_preprocessed.csv', sep='|'))\n",
    "df_test = shuffle(pd.read_csv('data/test_santander.csv', usecols=['id','Pregunta']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "132    600\n293    567\n1      493\n147    488\n223    477\n      ... \n11       3\n15       3\n13       3\n24       2\n104      1\nName: Intencion_cat_label, Length: 352, dtype: int64\n-----------------\nBefore Resample: \n-----------------\nTrain Shape: (20104, 7)\nPoor cases: 1\n                     Pregunta  Intencion  \\\nIntencion_cat_label                        \n104                         1          1   \n\n                     Preguntas_custom_preprocess_no_stopwords  \\\nIntencion_cat_label                                             \n104                                                         1   \n\n                     Preguntas_custom_preprocess  \\\nIntencion_cat_label                                \n104                                            1   \n\n                     Preguntas_custom_preprocess_w_verbs  \\\nIntencion_cat_label                                        \n104                                                    1   \n\n                     Preguntas_word_tokenize  \nIntencion_cat_label                           \n104                                        1  \n----------------\nAfter Resample: \n----------------\nTrain Shape: (20105, 7)\n"
    }
   ],
   "source": [
    "print(df_train['Intencion_cat_label'].value_counts())\n",
    "# add one more sample because I have one case with just one sample and stratify need at least 2 samples\n",
    "df_train = apply_resample(df_train, 'Pregunta', 2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-07-11T13:14:18.428157Z",
     "iopub.status.busy": "2020-07-11T13:14:18.428157Z",
     "iopub.status.idle": "2020-07-11T13:14:18.441167Z",
     "shell.execute_reply": "2020-07-11T13:14:18.440183Z",
     "shell.execute_reply.started": "2020-07-11T13:14:18.428157Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "C:\\ProgramData\\Anaconda3\\envs\\env-fsadosky\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
    }
   ],
   "source": [
    "X = df_train['Preguntas_word_tokenize'].values\n",
    "y = df_train['Intencion_cat_label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X), y, test_size = TEST_SIZE, stratify=y, random_state=RANDOM_STATE, shuffle=SHUFFLE)\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "C:\\ProgramData\\Anaconda3\\envs\\env-fsadosky\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
    }
   ],
   "source": [
    "if APPLY_RESAMPLE == True:\n",
    "    #Join X_train and y_train to add more classes to train\n",
    "\n",
    "    df_Xtrain = pd.DataFrame(X_train, columns=['Pregunta'])\n",
    "    df_Xtrain.set_index('Pregunta')\n",
    "    df_Xtrain['Intencion_cat_label'] = y_train\n",
    "    cnt_pro = df_Xtrain['Intencion_cat_label'].value_counts()\n",
    "\n",
    "    plt.figure(figsize=(35,4))\n",
    "    sns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\n",
    "    plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "    plt.xlabel('Intencion', fontsize=12)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "    #Apply resample to train dataset\n",
    "    df_Xtrain = apply_resample(df_Xtrain, 'Pregunta', 50, 10)\n",
    "\n",
    "    X_train = df_Xtrain['Pregunta'].values\n",
    "    y_train = df_Xtrain['Intencion_cat_label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-07-11T13:14:18.445176Z",
     "iopub.status.busy": "2020-07-11T13:14:18.444186Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\nPreprocessing data...\nTraining Model...\nFitting 5 folds for each of 1 candidates, totalling 5 fits\n"
    }
   ],
   "source": [
    "for classifier in classifier_list:\n",
    "    #try:\n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_param('Classifier', classifier.__class__.__name__)    \n",
    "            mlflow.log_param('resampling', APPLY_RESAMPLE)         \n",
    "            mlflow.log_param('vect', VECTORIZER_TYPE.name)\n",
    "            mlflow.log_param('tokenizer', ('None' if (TOKENIZER_TYPE == None) else TOKENIZER_TYPE.name))    \n",
    "            mlflow.log_param('resampling_class', oversample.__class__.__name__) \n",
    "            mlflow.log_param('sampling_strategy', SAMPLING_STRATEGY)   \n",
    "\n",
    "            optimized_model, model_best_params, X_train, X_test, y_train, y_test = GenerateTrainedModel(classifier, X_train, X_test, y_train, y_test)\n",
    "            pred = optimized_model.predict(X_test)\n",
    "            #Compute the balanced accuracy\n",
    "            #The balanced accuracy in binary and multiclass classification problems to deal with imbalanced datasets. It is defined as the average of recall obtained on each class.\n",
    "            #The best value is 1 and the worst value is 0 when adjusted=False.\n",
    "            print('Calculating Scores...')\n",
    "            balanced_accuracy_score = Accuracy.get_balanced_accuracy_score(y_test, pred)\n",
    "            accuracy_score = Accuracy.get_accuracy_score(y_test, pred)\n",
    "            \n",
    "            print('Train best model with all train data')\n",
    "            optimized_model = TrainModel(X, y, classifier)\n",
    "            \n",
    "            print('Processing Classification Report...')\n",
    "            report = Accuracy.get_classification_report(y_test, pred)\n",
    "            df_report = pd.DataFrame()\n",
    "            lines = report.split('\\n')\n",
    "            for line in lines[2:-5]:    \n",
    "                row_data = line.split('      ')             \n",
    "                row = pd.Series(data={'class': row_data[1].strip(), \n",
    "                                        'precision': row_data[2].strip(), \n",
    "                                        'recall': row_data[3].strip(), \n",
    "                                        'f1-score': row_data[4].strip(), \n",
    "                                        'support': row_data[5].strip()\n",
    "                                        }\n",
    "                                        )\n",
    "                df_report = df_report.append(row, ignore_index=True)\n",
    "            df_report['f1-score'] = df_report['f1-score'].astype(float)\n",
    "            df_report['precision'] = df_report['precision'].astype(float)\n",
    "            df_report['recall'] = df_report['recall'].astype(float)\n",
    "            df_report['class'] = df_report['class'].astype(int)\n",
    "            \n",
    "            print('Save predictions of test to CSV...')\n",
    "            pred_det = optimized_model.predict(df_test['Pregunta'].values)\n",
    "            df_test['Intencion'] = pred_det\n",
    "            SUBMIT_FILE = 'data/submit_{}.csv'.format(classifier.__class__.__name__)\n",
    "            df_test.to_csv(SUBMIT_FILE, mode='w', header=False, columns=['id','Intencion'], index=False, sep=',')\n",
    "\n",
    "            print('Logging metrics...')\n",
    "            #Save metrics\n",
    "            mlflow.log_metric(\"balanced_accuracy_score\", balanced_accuracy_score)\n",
    "            mlflow.log_metric(\"accuracy_score\", accuracy_score)                \n",
    "            for key, value in model_best_params.items():        \n",
    "                mlflow.log_param(key, value)\n",
    "            mlflow.sklearn.log_model(optimized_model, \"model\")\n",
    "            mlflow.log_metric(\"classification_report\", df_report)                \n",
    "    #except:\n",
    "    #    print('There was an error!')\n",
    "    #    print(sys.exc_info())\n",
    "    #    mlflow.end_run()\n",
    "    #    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "C:\\ProgramData\\Anaconda3\\envs\\env-fsadosky\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'df_report' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-038e4fb2b18d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcnt_pro\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_report\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_report\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'recall'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_figheight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_figwidth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_report' is not defined"
     ]
    }
   ],
   "source": [
    "cnt_pro = df_report[df_report['recall'] < 0.50]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, sharex=True)\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(15)\n",
    "sns.barplot(cnt_pro.index, cnt_pro.recall, alpha=0.8, ax=ax[0])\n",
    "plt.ylabel('Recall', fontsize=12)\n",
    "plt.xlabel('Intencion', fontsize=12)\n",
    "\n",
    "items = df_train[df_train['Intencion_cat_label'].isin(cnt_pro.index)]['Intencion_cat_label'].value_counts()\n",
    "\n",
    "sns.barplot(items.index, items.values, alpha=0.8, ax=ax[1])\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                Pregunta Intencion  \\\n29                        solicitud tarjeta credito visa   Cat_337   \n39             quiera solicitar la tarjeta santander rio   Cat_337   \n62               como obtengo mi primera tarjeta credito   Cat_337   \n151                         solicitar tarjeta de credito   Cat_337   \n196                             cobro jubilacion tarjeta   Cat_337   \n...                                                  ...       ...   \n19666                quiero tarjeta mastercard santander   Cat_337   \n19763                     necesito adquirir tarjeta visa   Cat_337   \n19820                           quiero tarjeta santander   Cat_337   \n20041  quisiera saber si puedo tener mi tarjeta de cr...   Cat_337   \n20094              solicitar tarjeta credito caja ahorro   Cat_337   \n\n       Intencion_cat_label           Preguntas_custom_preprocess_no_stopwords  \\\n29                     337           ['solicitud', 'tarjet', 'credit', 'vis']   \n39                     337  ['quier', 'solicit', 'la', 'tarjet', 'santand'...   \n62                     337  ['com', 'obteng', 'mi', 'primer', 'tarjet', 'c...   \n151                    337              ['solicit', 'tarjet', 'de', 'credit']   \n196                    337                   ['cobr', 'jubilacion', 'tarjet']   \n...                    ...                                                ...   \n19666                  337       ['quier', 'tarjet', 'mastercard', 'santand']   \n19763                  337             ['necesit', 'adquir', 'tarjet', 'vis']   \n19820                  337                     ['quier', 'tarjet', 'santand']   \n20041                  337  ['quis', 'sab', 'si', 'pued', 'ten', 'mi', 'ta...   \n20094                  337    ['solicit', 'tarjet', 'credit', 'caj', 'ahorr']   \n\n                             Preguntas_custom_preprocess  \\\n29              ['solicitud', 'tarjet', 'credit', 'vis']   \n39      ['quier', 'solicit', 'tarjet', 'santand', 'rio']   \n62              ['obteng', 'primer', 'tarjet', 'credit']   \n151                      ['solicit', 'tarjet', 'credit']   \n196                     ['cobr', 'jubilacion', 'tarjet']   \n...                                                  ...   \n19666       ['quier', 'tarjet', 'mastercard', 'santand']   \n19763             ['necesit', 'adquir', 'tarjet', 'vis']   \n19820                     ['quier', 'tarjet', 'santand']   \n20041  ['quis', 'sab', 'pued', 'ten', 'tarjet', 'cred...   \n20094    ['solicit', 'tarjet', 'credit', 'caj', 'ahorr']   \n\n                     Preguntas_custom_preprocess_w_verbs  \\\n29              ['solicitud', 'tarjet', 'credit', 'vis']   \n39      ['quier', 'solicit', 'tarjet', 'santand', 'rio']   \n62              ['obteng', 'primer', 'tarjet', 'credit']   \n151                      ['solicit', 'tarjet', 'credit']   \n196                     ['cobr', 'jubilacion', 'tarjet']   \n...                                                  ...   \n19666        ['quer', 'tarjet', 'mastercard', 'santand']   \n19763             ['necesit', 'adquir', 'tarjet', 'vis']   \n19820                      ['quer', 'tarjet', 'santand']   \n20041  ['quis', 'sab', 'pued', 'ten', 'tarjet', 'cred...   \n20094    ['solicit', 'tarjet', 'credit', 'caj', 'ahorr']   \n\n                                 Preguntas_word_tokenize  \n29           ['solicitud', 'tarjeta', 'credito', 'visa']  \n39     ['quiera', 'solicitar', 'la', 'tarjeta', 'sant...  \n62     ['como', 'obtengo', 'mi', 'primera', 'tarjeta'...  \n151            ['solicitar', 'tarjeta', 'de', 'credito']  \n196                   ['cobro', 'jubilacion', 'tarjeta']  \n...                                                  ...  \n19666   ['quiero', 'tarjeta', 'mastercard', 'santander']  \n19763        ['necesito', 'adquirir', 'tarjeta', 'visa']  \n19820                 ['quiero', 'tarjeta', 'santander']  \n20041  ['quisiera', 'saber', 'si', 'puedo', 'tener', ...  \n20094  ['solicitar', 'tarjeta', 'credito', 'caja', 'a...  \n\n[304 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pregunta</th>\n      <th>Intencion</th>\n      <th>Intencion_cat_label</th>\n      <th>Preguntas_custom_preprocess_no_stopwords</th>\n      <th>Preguntas_custom_preprocess</th>\n      <th>Preguntas_custom_preprocess_w_verbs</th>\n      <th>Preguntas_word_tokenize</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>29</th>\n      <td>solicitud tarjeta credito visa</td>\n      <td>Cat_337</td>\n      <td>337</td>\n      <td>['solicitud', 'tarjet', 'credit', 'vis']</td>\n      <td>['solicitud', 'tarjet', 'credit', 'vis']</td>\n      <td>['solicitud', 'tarjet', 'credit', 'vis']</td>\n      <td>['solicitud', 'tarjeta', 'credito', 'visa']</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>quiera solicitar la tarjeta santander rio</td>\n      <td>Cat_337</td>\n      <td>337</td>\n      <td>['quier', 'solicit', 'la', 'tarjet', 'santand'...</td>\n      <td>['quier', 'solicit', 'tarjet', 'santand', 'rio']</td>\n      <td>['quier', 'solicit', 'tarjet', 'santand', 'rio']</td>\n      <td>['quiera', 'solicitar', 'la', 'tarjeta', 'sant...</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>como obtengo mi primera tarjeta credito</td>\n      <td>Cat_337</td>\n      <td>337</td>\n      <td>['com', 'obteng', 'mi', 'primer', 'tarjet', 'c...</td>\n      <td>['obteng', 'primer', 'tarjet', 'credit']</td>\n      <td>['obteng', 'primer', 'tarjet', 'credit']</td>\n      <td>['como', 'obtengo', 'mi', 'primera', 'tarjeta'...</td>\n    </tr>\n    <tr>\n      <th>151</th>\n      <td>solicitar tarjeta de credito</td>\n      <td>Cat_337</td>\n      <td>337</td>\n      <td>['solicit', 'tarjet', 'de', 'credit']</td>\n      <td>['solicit', 'tarjet', 'credit']</td>\n      <td>['solicit', 'tarjet', 'credit']</td>\n      <td>['solicitar', 'tarjeta', 'de', 'credito']</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>cobro jubilacion tarjeta</td>\n      <td>Cat_337</td>\n      <td>337</td>\n      <td>['cobr', 'jubilacion', 'tarjet']</td>\n      <td>['cobr', 'jubilacion', 'tarjet']</td>\n      <td>['cobr', 'jubilacion', 'tarjet']</td>\n      <td>['cobro', 'jubilacion', 'tarjeta']</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19666</th>\n      <td>quiero tarjeta mastercard santander</td>\n      <td>Cat_337</td>\n      <td>337</td>\n      <td>['quier', 'tarjet', 'mastercard', 'santand']</td>\n      <td>['quier', 'tarjet', 'mastercard', 'santand']</td>\n      <td>['quer', 'tarjet', 'mastercard', 'santand']</td>\n      <td>['quiero', 'tarjeta', 'mastercard', 'santander']</td>\n    </tr>\n    <tr>\n      <th>19763</th>\n      <td>necesito adquirir tarjeta visa</td>\n      <td>Cat_337</td>\n      <td>337</td>\n      <td>['necesit', 'adquir', 'tarjet', 'vis']</td>\n      <td>['necesit', 'adquir', 'tarjet', 'vis']</td>\n      <td>['necesit', 'adquir', 'tarjet', 'vis']</td>\n      <td>['necesito', 'adquirir', 'tarjeta', 'visa']</td>\n    </tr>\n    <tr>\n      <th>19820</th>\n      <td>quiero tarjeta santander</td>\n      <td>Cat_337</td>\n      <td>337</td>\n      <td>['quier', 'tarjet', 'santand']</td>\n      <td>['quier', 'tarjet', 'santand']</td>\n      <td>['quer', 'tarjet', 'santand']</td>\n      <td>['quiero', 'tarjeta', 'santander']</td>\n    </tr>\n    <tr>\n      <th>20041</th>\n      <td>quisiera saber si puedo tener mi tarjeta de cr...</td>\n      <td>Cat_337</td>\n      <td>337</td>\n      <td>['quis', 'sab', 'si', 'pued', 'ten', 'mi', 'ta...</td>\n      <td>['quis', 'sab', 'pued', 'ten', 'tarjet', 'cred...</td>\n      <td>['quis', 'sab', 'pued', 'ten', 'tarjet', 'cred...</td>\n      <td>['quisiera', 'saber', 'si', 'puedo', 'tener', ...</td>\n    </tr>\n    <tr>\n      <th>20094</th>\n      <td>solicitar tarjeta credito caja ahorro</td>\n      <td>Cat_337</td>\n      <td>337</td>\n      <td>['solicit', 'tarjet', 'credit', 'caj', 'ahorr']</td>\n      <td>['solicit', 'tarjet', 'credit', 'caj', 'ahorr']</td>\n      <td>['solicit', 'tarjet', 'credit', 'caj', 'ahorr']</td>\n      <td>['solicitar', 'tarjeta', 'credito', 'caja', 'a...</td>\n    </tr>\n  </tbody>\n</table>\n<p>304 rows × 7 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "poor_values = df_train[df_train.Intencion_cat_label == 337]\n",
    "poor_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('env-fsadosky': conda)",
   "language": "python",
   "name": "python_defaultSpec_1600687268913"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}